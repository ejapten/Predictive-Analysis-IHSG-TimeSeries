# -*- coding: utf-8 -*-
"""Timeseries pada IHSG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jyu1Lpbc7d7q_mDUuQBccZBTjwaHyvgt

# **Analisis dan Prediksi IHSG Menggunakan Pendekatan Time Series**

Tujuan :  membuat model yang dapat memprediksi IHSG yang memanfaatkan data historis sehingga dapat memberikan gambaran awal mengenai pergerakan indeks IHSG.

# Import Library

Instalisasi Library yang akan digunakan untuk mendukung proses analisis ini
"""

pip install keras-tuner

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Dense, LSTM
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import keras_tuner as kt

"""# Load and Understanding Data

Sumber dataseet IHSG : https://id.investing.com/indices/idx-composite

Dataset dimuat berikut:
- data_ihsg1 : dataset IHSG dari tahun 1990-2010
- data_ihsg2 : dataset IHSG dari tahun 2011- mei 2025
"""

data_ihsg1 = pd.read_csv("https://raw.githubusercontent.com/ejapten/Predictive-Analysis-IHSG-TimeSeries/refs/heads/main/raw_dataset/1990_2010_IHSG.csv")
data_ihsg2 = pd.read_csv("https://raw.githubusercontent.com/ejapten/Predictive-Analysis-IHSG-TimeSeries/refs/heads/main/raw_dataset/2011_2025_IHSG%20.csv")

"""dataset digabungkan karena konteks data tersebut sama dan memudahkan untuk menganalisis data IHSG, dataset IHSG tersebut sama hanya saja periode waktu yang berbeda. Dataset yang digabungkan akan di-inisialisasi dengan data_ihsg agar mudah dalam tahap EDA"""

data_ihsg = pd.concat([data_ihsg1,data_ihsg2])
data_ihsg

data_ihsg.shape

"""data setelah dibersihkan berjumlah 7816 dan 3 variable"""

data_ihsg.info()

"""Setelah mengecek tipe data, ternyata semua variable bertipe data object"""

data_ihsg.isnull().sum()

"""Hanya variable Vol. saja yang memiliki Missing Value"""

data_ihsg.duplicated().sum()

"""Dalam data tersebut tidak memilik duplikat data

# Data Preparation

Dalamm proses load dan understading Data sudah dilakukan penggabungan data IHSG versi 1990-2010 dan vers 2011 - Mei 2025 menggunakan Concat. Oleh karena itu, daalam data preparation akan menggunakan data_ihsg yang sudah digabungkan dari penggabungan data IHSG versi 1990-2010 dan versi 2011 - Mei 2025. Atau  bisa inisalisasi ulang menggunakan concat
"""

data_ihsg = pd.concat([data_ihsg1,data_ihsg2])
data_ihsg

"""Pada tahap Load dan Understanding Data, sudah mengetahui tuh apa saja yang harus dibersihkan, langkah selanjutnya adalah pembersihan data dan prepare data untuk modeling nanti

Membuat fungsi cleaning untuk membersihkan data:
- Tujuan pada analisis proyek ini adalah melihat pergerakan IHSG di masa depan berdasarkan Price atau Harga. Maka fitur selain Price dan Date akan dihapus
- Tidak akan menghapus missing value pada karena variabel Date dan Price tidak ada yg kosong, melainkan yang kosong ada pada variabel Vol. dimana variabel Vol. akan dihapus
"""

# Fungsi menghapus variable/Kolom
def delete_columns(df, columns_to_delete):
  del_columns = df.drop(columns_to_delete, axis=1)
  return del_columns

#  Fungsi untuk konversi tipe data menjadi tanggal
def convert_to_datetime(df, col_name):
  df[col_name] = pd.to_datetime(df[col_name], format="%m/%d/%Y")
  return df
# Fungsi Untuk mengurutkan tanggal
def convert_and_sort_datetime(df, col_name):
  df[col_name] = pd.to_datetime(df[col_name], format="%m/%d/%Y")
  df = df.sort_values(by=col_name)
  df = df.reset_index(drop=True)
  return df

# Fungsi untuk konversi tipe data menjadi float dimana nilai unique sebelumnya bersifat
def convert_price_to_float(df, col_name):
  df[col_name] = df[col_name].str.replace(",", "").astype(float)
  return df

# Apply Fungsi Cleaning
columns_to_delete = ["Open", "High", "Low", "Vol.","Change %"] # Variable yang akan di delete

data_ihsg = delete_columns(data_ihsg, columns_to_delete)
data_ihsg = convert_to_datetime(data_ihsg, "Date")
data_ihsg = convert_and_sort_datetime(data_ihsg, "Date")
data_ihsg = convert_price_to_float(data_ihsg, "Price")

"""Fungsi cleaning diatas digunakan untuk membersihkan data
- Variable Open, High, Low, Vol., dan Change % akan dihapus karena tidak akan dilakukan proses analisis pada proyek kali ini. Namun, Price akan dilakukan analisis, dimana Price aka dijadikan sebagai variabel yang akan memprediksi pergerakan IHSG.

- Lalu dilakukan konversi tipe data menjadi datetime pada variabel Date dan diurutkan

Variable Price akan distandarisasikan dalam rentang 0 sampai 1 menggunakan Min-Max Scaling
"""

scaler = MinMaxScaler()
data_ihsg["Price"] = scaler.fit_transform(data_ihsg[["Price"]])

"""Memuat price sebagai bentuk array dari variable Price"""

date = data_ihsg['Date'].values
price  =data_ihsg['Price'].values

"""Membuat data train dan data validasi untuk proses modelling dan mengevaluasi hasil model"""

# 80% untuk train dan 20% untuk validasi
split_time = int(len(price) * 0.8)
train_series = price[:split_time]
val_series = price[split_time:]

"""Membuat fungsi windowed_dataset_multi dimana Fungsi ini membagi data time series menjadi window input dan target output untuk keperluan pelatihan model multi-step forecasting."""

def windowed_dataset_multi(series, window_size, output_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + output_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + output_size))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:window_size], w[window_size:]))
    return ds.batch(batch_size).prefetch(1)

"""Membentuk train_set dan val_set dengan window input 30 dan target output 60, dibagi dalam batch dan diacak agar siap digunakan untuk pelatihan model"""

window_size = 30
output_size = 60
batch_size=100
shuffle_buffer=1000

train_set = windowed_dataset_multi(train_series, window_size, output_size, batch_size, shuffle_buffer)
val_set = windowed_dataset_multi(val_series, window_size, output_size, batch_size, shuffle_buffer)

"""# Modelling

### LSTM without Tunning
"""

# Membangun model LSTM untuk prediksi multi-step IHSG dengan 3 lapis layer
model_lstm = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(32),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(60),
])
# Optimizer Adam dengan learning rate kecil
optimizer = tf.keras.optimizers.Adam(learning_rate=1.0000e-04)

# dengan patience 5 akan mengehntikan training saat val_loss tidak membaik selama 5 epoch
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

# Simpan model terbaik berdasarkan val_loss
model_ckpt = ModelCheckpoint('best_model_lstm.h5', monitor='val_loss', save_best_only=True, verbose=1)

# Callback saat dipake pada training
callbacks = [early_stop, model_ckpt]

# Compile model dengan loss MSE dan metric MAE
model_lstm.compile(loss='mse', optimizer=optimizer, metrics=["mae"])

# training model

history_lstm = model_lstm.fit(train_set,validation_data=val_set, epochs=100, callbacks=callbacks)

"""### LSTM With Tunning"""

def build_model(hp):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.LSTM(
        hp.Int("lstm_units_1", min_value=32, max_value=128, step=32),
        return_sequences=True
    ))
    model.add(tf.keras.layers.LSTM(
        hp.Int("lstm_units_2", min_value=16, max_value=64, step=16)
    ))
    model.add(tf.keras.layers.Dense(
        hp.Int("dense_units", min_value=32, max_value=128, step=32),
        activation="relu"
    ))
    model.add(tf.keras.layers.Dense(output_size))

    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Choice("learning_rate", [1e-4, 1e-3])
        ),
        loss="mse",
        metrics=["mae"]
    )

    return model

tuner = kt.GridSearch(
    build_model,
    objective="val_loss",
    max_trials=3,
    executions_per_trial=1,
    overwrite=True,
    directory="tuner_dir",
    project_name="ihsg_lstm"
)
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

# Simpan model terbaik berdasarkan val_loss
model_ckpt = ModelCheckpoint('best_model_lstm_with_tunnin.h5', monitor='val_loss', save_best_only=True, verbose=1)

tuner.search(train_set, validation_data=val_set, epochs=50, callbacks=[early_stop])

"""kode diatas adalah menjallankan proses pencarian hyperparameter terbaik untuk model yang telah dibuat oleh build_model()"""

best_hps = tuner.get_best_hyperparameters(1)[0]
best_model = tuner.hypermodel.build(best_hps)
print("Best LSTM units 1:", best_hps.get("lstm_units_1"))
print("Best LSTM units 2:", best_hps.get("lstm_units_2"))
print("Best Dense units:", best_hps.get("dense_units"))
print("Best Learning rate:", best_hps.get("learning_rate"))

"""Hasil hyperparamter terbaiik adalah
- Layer LSTM pertama menggunakan 32 neuron.
- Layer LSTM kedua menggunakan 16 neuron.
- Layer Dense menggunakan 32 neuron.
- Optimizer Adam menggunakan learning rate 0.001, yang lebih cepat daripada 0.0001, dan memberikan hasil terbaik.
"""

history_lstm_with_tunning = best_model.fit(train_set, validation_data=val_set, epochs=100, callbacks = [early_stop, model_ckpt])

"""# Evaluasi  dan Result

### evaluasi
"""

# Melihat grafik kurva Loss
plt.plot(history_lstm.history['loss'], label='Training Loss')
plt.plot(history_lstm.history['val_loss'], label='Validation Loss')
plt.title('Loss Curve LSTM Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Mengetahaui perbandingan skor train dan val
train_score_lstm = model_lstm.evaluate(train_set)
val_score_lstm = model_lstm.evaluate(val_set)

"""**Analisis Grafik:**
- Train dan validation loss sama sama turun dratis terus mendatar di nilai yang sangat kecil
- Tidak ada gap besar
- Validation loss sedikit lebih tinggi dari training loss, tapi polanya mengikuti dan stabil, tidak naik tajam di akhir epoch.

**Analisis Score:**
- nilai  loss dan mae pada training dan validasi sama sama kecil
- mae validasi sedikit lebih tinggi dari training tapi gap nya tidak besar
- Tidak ada tanda overfitting (yaitu training loss jauh lebih kecil dari validation loss).
- Tidak ada tanda underfitting (yaitu loss tidak turun atau tetap tinggi).

**Kesimpulan:**
- Model LSTM Tidak Overfitting dan Underfitting
- Model baik dan bisa digunakan untuk prediksi



"""

# Melihat grafik kurva Loss
plt.plot(history_lstm_with_tunning.history['loss'], label='Training Loss')
plt.plot(history_lstm_with_tunning.history['val_loss'], label='Validation Loss')
plt.title('Loss Curve LSTM with Tunning Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Mengetahaui perbandingan skor train dan val
train_score_lstm_tunning =best_model.evaluate(train_set)
val_score_lstm_tunning = best_model.evaluate(val_set)

"""**Analisis Grafik:**
- Train dan validation loss sama sama turun dengan cepaat
- Tidak ada gap besar
- Validation loss juga turun di awal, lalu fluktuasi dan lebih tinggi dari training loss.

**Analisis Score:**
- nilai  loss dan mae pada training dan validasi sama sama kecil
- mae validasi sedikit lebih tinggi dari training tapi gap nya tidak besar
- Tidak ada tanda overfitting (yaitu training loss jauh lebih kecil dari validation loss).
- Validation MAE juga masih cukup rendah.

**Kesimpulan:**
- Model LSTM Tidak Overfitting dan Underfitting
- Model baik dan bisa digunakan untuk prediksi

**Perbandingan**
"""

mae_scores = {
    'LSTM Train': train_score_lstm[1],
    'LSTM Val': val_score_lstm[1],
    'LSTM with Tunning Train': train_score_lstm_tunning[1],
    'LSTM with Tunning Val': val_score_lstm_tunning[1]
}

plt.figure(figsize=(12, 5))
plt.bar(mae_scores.keys(), mae_scores.values(), color=['skyblue', 'blue', 'pink', 'red'])
plt.ylabel('MAE')
plt.title('Perbandingan MAE LSTM vs MAE LSTM With Tunning (Train & Validation)')
plt.ylim(0, max(mae_scores.values()) + 0.01)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""Model LSTM with Tunning memiliki MAE yang lebih rendah dari LSTM without tunning baik dari train dan validation. maka LSTM with Tunning model yang paling baik untuk prediksi

### Result

**Melakukan prediksi selama 60 hari kedepan**
"""

# Ambil 60 data terakhir dari data asli (seluruh price yang sudah diskalakan)
last_window = price[-60:]
last_window = np.expand_dims(last_window, axis=(0, 2))

# Lakukan prediksi ke depan 60 hari menggunkan model cnn
future_pred_scaled = best_model.predict(last_window)
future_pred_scaled = future_pred_scaled.flatten()

# Inverse hasil prediksi ke skala asli
future_pred = scaler.inverse_transform(future_pred_scaled.reshape(-1, 1)).flatten()

# Buat range tanggal prediksi 60 hari ke depan
last_date = pd.to_datetime(data_ihsg["Date"].iloc[-1])
future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 61)]

# Simpan hasil prediksi ke DataFrame
future_df = pd.DataFrame({
    "Date": future_dates,
    "Predicted_IHSG": future_pred
})

future_df.head(5)

"""**Visualisasi Hasil**"""

# Inverse -- cukup 1x run
data_ihsg["Price"] = scaler.inverse_transform(data_ihsg[["Price"]])

# Ambil 60 hari terakhir data historis
historical_60 = data_ihsg.tail(60).copy()

# Gabungkan semua tanggal (historis dan prediksi)
all_dates = pd.concat([historical_60['Date'], future_df['Date']])

plt.figure(figsize=(20, 5))
plt.plot(historical_60['Date'], historical_60['Price'], label='Historical IHSG (60 days)', marker='o')
plt.plot(future_df['Date'], future_df['Predicted_IHSG'], label='Predicted IHSG (60 days)', marker='o')

plt.xlabel('Date')
plt.ylabel('IHSG Price')
plt.title('Historical and Predicted IHSG Prices')
plt.legend()
plt.grid(True)

# Tampilkan semua tanggal pada sumbu X secara jelas
plt.xticks(ticks=all_dates, labels=all_dates.dt.strftime('%Y-%m-%d'), rotation=90)

plt.tight_layout()
plt.show()

"""pada line orange merupakan Hasil dari Prediksi selama 60 hari"""

# Ambil 60 hari terakhir data historis (harga asli)
# historical_60 = data_ihsg.tail(60).copy()

# Data prediksi 60 hari ke depan sudah ada di future_df (kolom "Predicted_IHSG")
# Plot data historis dan prediksi
plt.figure(figsize=(25, 4))
plt.plot(data_ihsg['Date'], data_ihsg['Price'], label='Historical IHSG')
plt.plot(future_df['Date'], future_df['Predicted_IHSG'], label='Predicted IHSG (60 days)')
plt.xlabel('Date')
plt.ylabel('IHSG Price')
plt.title('Historical and Predicted IHSG Prices')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()