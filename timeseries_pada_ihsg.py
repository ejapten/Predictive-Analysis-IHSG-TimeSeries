# -*- coding: utf-8 -*-
"""Timeseries pada IHSG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jyu1Lpbc7d7q_mDUuQBccZBTjwaHyvgt

# **Analisis dan Prediksi IHSG Menggunakan Pendekatan Time Series**

Tujuan :  membuat model yang dapat memprediksi IHSG yang memanfaatkan data historis sehingga dapat memberikan gambaran awal mengenai pergerakan indeks IHSG.

# Import Library

Instalisasi Library yang akan digunakan untuk mendukung proses analisis ini
"""

pip install keras-tuner

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Dense, LSTM
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import keras_tuner as kt

"""# Load and Understanding Data

Sumber dataseet IHSG : https://id.investing.com/indices/idx-composite

Dataset dimuat berikut:
- data_ihsg1 : dataset IHSG dari tahun 1990-2010
- data_ihsg2 : dataset IHSG dari tahun 2011- mei 2025
"""

data_ihsg1 = pd.read_csv("https://raw.githubusercontent.com/ejapten/Predictive-Analysis-IHSG-TimeSeries/refs/heads/main/raw_dataset/1990_2010_IHSG.csv")
data_ihsg2 = pd.read_csv("https://raw.githubusercontent.com/ejapten/Predictive-Analysis-IHSG-TimeSeries/refs/heads/main/raw_dataset/2011_2025_IHSG%20.csv")

"""dataset digabungkan karena konteks data tersebut sama dan memudahkan untuk menganalisis data IHSG, dataset IHSG tersebut sama hanya saja periode waktu yang berda"""

data_ihsg = pd.concat([data_ihsg1,data_ihsg2])
data_ihsg

data_ihsg.info()

"""Setelah mengecek tipe data, ternyata semua variable bertipe data object"""

data_ihsg.isnull().sum()

"""Hanya variable Vol. saja yang memiliki Missing Value"""

data_ihsg.duplicated().sum()

"""Dalam data tersebut tidak memilik duplikat data

# Cleaning Data

Membuat fungsi cleaning untuk membersihkan data
"""

# Fungsi menghapus missing value
def delete_missing_value(df):
  cleaned_mv = df.dropna()
  return cleaned_mv

# Fungsi menghapus variable/Kolom
def delete_columns(df, columns_to_delete):
  del_columns = df.drop(columns_to_delete, axis=1)
  return del_columns

#  Fungsi untuk konversi tipe data menjadi tanggal
def convert_to_datetime(df, col_name):
  df[col_name] = pd.to_datetime(df[col_name], format="%m/%d/%Y")
  return df
# Fungsi Untuk mengurutkan tanggal
def convert_and_sort_datetime(df, col_name):
  df[col_name] = pd.to_datetime(df[col_name], format="%m/%d/%Y")
  df = df.sort_values(by=col_name)
  df = df.reset_index(drop=True)
  return df

# Fungsi untuk konversi tipe data menjadi float dimana nilai unique sebelumnya bersifat
def convert_price_to_float(df, col_name):
  df[col_name] = df[col_name].str.replace(",", "").astype(float)
  return df
# Fungsi Untuk konversi tipe data menjadi float dimana nilai unique sebelumnya memiliki simbol %
def convert_percent_to_float(df, col_name):
    df[col_name] = df[col_name].str.replace("%", "").astype(float) / 100
    return df

# Apply Fungsi Cleaning
columns_to_delete = ["Open", "High", "Low", "Vol."] # Variable yang akan di delete

data_ihsg = delete_missing_value(data_ihsg)
data_ihsg = delete_columns(data_ihsg, columns_to_delete)
data_ihsg = convert_to_datetime(data_ihsg, "Date")
data_ihsg = convert_and_sort_datetime(data_ihsg, "Date")
data_ihsg = convert_price_to_float(data_ihsg, "Price")
data_ihsg = convert_percent_to_float(data_ihsg, "Change %")

"""Fungsi cleaning diatas digunakan untuk membersihkan data
- Variable Open, High, Low, Vol akn dihapus karena tidak akan dilakukan proses analisis pada proyek kali ini. Namun, Price dan Change % akan dilakukan analisis, dimana Price aka dijadikan sebagai variabel yang akan memprediksi pergerakan IHSG. Sedangkan Change % tidak akan dilakukan prediksi namun digunakan untuk melihat pola pada data IHSG

- Lalu dilakukan konversi tipe data menjadi datetime pada variabel Date dan diurutkan

- Konvers tipe data dilakukan pada Price dan Change % menjadi floatt

# EDA
"""

data_ihsg.shape

"""data setelah dibersihkan berjumlah 7816 dan 3 variable"""

# Melihat outliers
kolom_numerik_eda = data_ihsg.select_dtypes(include='number').columns
# Buat grid
num_cols = len(kolom_numerik_eda)
num_rows = math.ceil(num_cols / 2)
# Visualisasi
plt.figure(figsize=(6, 4 * num_rows))
for i, column in enumerate(kolom_numerik_eda):
    plt.subplot(num_rows, 2, i + 1)
    sns.boxplot(y=data_ihsg[column])
plt.tight_layout()
plt.show()

"""Dalam variable Price tidak ada Outliers, namun pada variable Change % terdapat outliers. Namun Tidak perlu khawatir terhadap variable Change % karena tidak akan dilakukan prediksi"""

# Melihat persebaran data
num_cols_numeric = len(kolom_numerik_eda)
num_rows_numeric = math.ceil(num_cols_numeric / 2)

plt.figure(figsize=(6, 4 * num_rows_numeric))

for i, column in enumerate(kolom_numerik_eda):
    plt.subplot(num_rows_numeric, 2, i + 1)
    sns.histplot(data=data_ihsg, x=column, kde=True)

plt.tight_layout()
plt.show()

"""Variable Price berdistribusi tidak normal dan Change % normal"""

data_ihsg.describe()

"""Dalam data tersebut Tahun dimana data ini dimulai adalah 1993. Lalu Price atau harga IHSG paling tinggi pada angka 7905.39000 dan paling rendah dalam angka 256.83000 Lalu pada Change yang merupakan perubahan pernah turun mencapai -0.11

"""

# Melihat Jumlah Price yang paling banyak berdasarkan Waktu
High_price_based_on_date = (
    data_ihsg
    .groupby("Date")
    .agg({"Price": "sum"})
    .sort_values("Price", ascending=False)
    .head(5)
)
High_price_based_on_date

"""Ternyata Harga IHSG tertingi dari 1990-2025 berada pada tahun 2024 dan sama sama di bulan 9"""

# Melihat Jumlah Price yang paling sedikit berdasarkan Waktu
low_price_based_on_date = (
    data_ihsg
    .groupby("Date")
    .agg({"Price": "sum"})
    .sort_values("Price", ascending=True)
    .head(5)
)
low_price_based_on_date

"""  Sedangkan, Harga IHSG terendah dari 1990-2025 berada pada tahun 1998"""

plt.figure(figsize=(25, 6))
sns.lineplot(data=data_ihsg, x="Date", y="Price", palette="tab10")

plt.title("Fluktuasi Price IHSG")
plt.xlabel("Date")
plt.ylabel("Price")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""Berdasarkan visualisasi diatas dapat terlihat jelas bahwa IHSG terjadi penurunan pada rentang 1996-2000 dimana kemunginan besar adalah pada 1998. Laluu terjadi penurunan lagi pada rentang 2008-2012 secara tajam.Penurunan secara tajam lagi terjadi pada 2020 hingga kemungkinan 2022. Lalu pada 2025 terjadi penurunan secara tajam lagi pada bulan maret

Selain penurunan, pada 2000 terjadi kenaikan IHSG, lalu pada periode 2011 kemungkinan besar juga mengalami kenaikkan. Lalu rentang 2016-2020 terjadi kenaikan harga yang paling tinggi diantara tahun sebelumnya. Sebelum IHSG merosot turun pada maret 2025 IHSG pernah mengalami kenaikan
"""

plt.figure(figsize=(35, 4))
sns.lineplot(data=data_ihsg, x="Date", y="Change %", palette="tab10")
plt.title("Fluktuasi Price IHSG")
plt.xlabel("Date")
plt.ylabel("Volume")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""Berdasarkan persentase perubahan harga IHSG, terjadi naik turun setiap hari nya, jika dilihat periode 1996-2000 terjadi persentase kenaikan dan penurunan yang paling tajam dari tahun lainnya.

# Data Preparation

Variable Price akan distandarisasikan dalam rentang 0 sampai 1 menggunakan Min-Max Scaling
"""

scaler = MinMaxScaler()
data_ihsg["Price"] = scaler.fit_transform(data_ihsg[["Price"]])

"""Memuat price sebagai bentuk array dari variable Price"""

date = data_ihsg['Date'].values
price  =data_ihsg['Price'].values

"""Membuat data train dan data validasi untuk proses modelling dan mengevaluasi hasil model"""

# 80% untuk train dan 20% untuk validasi
split_time = int(len(price) * 0.8)
train_series = price[:split_time]
val_series = price[split_time:]

"""Membuat fungsi windowed_dataset_multi dimana Fungsi ini membagi data time series menjadi window input dan target output untuk keperluan pelatihan model multi-step forecasting."""

def windowed_dataset_multi(series, window_size, output_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + output_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + output_size))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:window_size], w[window_size:]))
    return ds.batch(batch_size).prefetch(1)

"""Membentuk train_set dan val_set dengan window input 30 dan target output 60, dibagi dalam batch dan diacak agar siap digunakan untuk pelatihan model"""

window_size = 30
output_size = 60
batch_size=100
shuffle_buffer=1000

train_set = windowed_dataset_multi(train_series, window_size, output_size, batch_size, shuffle_buffer)
val_set = windowed_dataset_multi(val_series, window_size, output_size, batch_size, shuffle_buffer)

"""# Modelling

### LSTM without Tunning
"""

# Membangun model LSTM untuk prediksi multi-step IHSG dengan 3 lapis layer
model_lstm = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(32),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(60),
])
# Optimizer Adam dengan learning rate kecil
optimizer = tf.keras.optimizers.Adam(learning_rate=1.0000e-04)

# dengan patience 5 akan mengehntikan training saat val_loss tidak membaik selama 5 epoch
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

# Simpan model terbaik berdasarkan val_loss
model_ckpt = ModelCheckpoint('best_model_lstm.h5', monitor='val_loss', save_best_only=True, verbose=1)

# Callback saat dipake pada training
callbacks = [early_stop, model_ckpt]

# Compile model dengan loss MSE dan metric MAE
model_lstm.compile(loss='mse', optimizer=optimizer, metrics=["mae"])

# training model

history_lstm = model_lstm.fit(train_set,validation_data=val_set, epochs=100, callbacks=callbacks)

"""### LSTM With Tunning"""

def build_model(hp):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.LSTM(
        hp.Int("lstm_units_1", min_value=32, max_value=128, step=32),
        return_sequences=True
    ))
    model.add(tf.keras.layers.LSTM(
        hp.Int("lstm_units_2", min_value=16, max_value=64, step=16)
    ))
    model.add(tf.keras.layers.Dense(
        hp.Int("dense_units", min_value=32, max_value=128, step=32),
        activation="relu"
    ))
    model.add(tf.keras.layers.Dense(output_size))

    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Choice("learning_rate", [1e-4, 1e-3])
        ),
        loss="mse",
        metrics=["mae"]
    )

    return model

tuner = kt.GridSearch(
    build_model,
    objective="val_loss",
    max_trials=3,
    executions_per_trial=1,
    overwrite=True,
    directory="tuner_dir",
    project_name="ihsg_lstm"
)
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

# Simpan model terbaik berdasarkan val_loss
model_ckpt = ModelCheckpoint('best_model_lstm_with_tunnin.h5', monitor='val_loss', save_best_only=True, verbose=1)

tuner.search(train_set, validation_data=val_set, epochs=50, callbacks=[early_stop])

"""kode diatas adalah menjallankan proses pencarian hyperparamtere terbaik untuk model yang telah dibuat oleh build_model()"""

best_hps = tuner.get_best_hyperparameters(1)[0]
best_model = tuner.hypermodel.build(best_hps)
print("Best LSTM units 1:", best_hps.get("lstm_units_1"))
print("Best LSTM units 2:", best_hps.get("lstm_units_2"))
print("Best Dense units:", best_hps.get("dense_units"))
print("Best Learning rate:", best_hps.get("learning_rate"))

"""Hasil hyperparamter terbaiik adalah
- Layer LSTM pertama menggunakan 32 neuron.
- Layer LSTM kedua menggunakan 16 neuron.
- Layer Dense menggunakan 32 neuron.
- Optimizer Adam menggunakan learning rate 0.001, yang lebih cepat daripada 0.0001, dan memberikan hasil terbaik.
"""

history_lstm_with_tunning = best_model.fit(train_set, validation_data=val_set, epochs=100, callbacks = [early_stop, model_ckpt])

"""# Evaluasi"""

# Melihat grafik kurva Loss
plt.plot(history_lstm.history['loss'], label='Training Loss')
plt.plot(history_lstm.history['val_loss'], label='Validation Loss')
plt.title('Loss Curve LSTM Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Mengetahaui perbandingan skor train dan val
train_score_lstm = model_lstm.evaluate(train_set)
val_score_lstm = model_lstm.evaluate(val_set)

"""**Analisis Grafik:**
- Train dan validation loss sama sama turun dratis terus mendatar di nilai yang sangat kecil
- Tidak ada gap besar
- Validation loss sedikit lebih tinggi dari training loss, tapi polanya mengikuti dan stabil, tidak naik tajam di akhir epoch.

**Analisis Score:**
- nilai  loss dan mae pada training dan validasi sama sama kecil
- mae validasi sedikit lebih tinggi dari training tapi gap nya tidak besar
- Tidak ada tanda overfitting (yaitu training loss jauh lebih kecil dari validation loss).
- Tidak ada tanda underfitting (yaitu loss tidak turun atau tetap tinggi).

**Kesimpulan:**
- Model LSTM Tidak Overfitting dan Underfitting
- Model baik dan bisa digunakan untuk prediksi



"""

# Melihat grafik kurva Loss
plt.plot(history_lstm_with_tunning.history['loss'], label='Training Loss')
plt.plot(history_lstm_with_tunning.history['val_loss'], label='Validation Loss')
plt.title('Loss Curve LSTM with Tunning Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Mengetahaui perbandingan skor train dan val
train_score_lstm_tunning =best_model.evaluate(train_set)
val_score_lstm_tunning = best_model.evaluate(val_set)

"""**Analisis Grafik:**
- Train dan validation loss sama sama turun dengan cepaat dan kemudian mendatar di angka kecil
- Tidak ada gap besar
- Validation loss sedikit lebih tinggi dari training loss, tapi polanya mengikuti dan stabil, tidak naik tajam di akhir epoch.

**Analisis Score:**
- nilai  loss dan mae pada training dan validasi sama sama kecil
- mae validasi sedikit lebih tinggi dari training tapi gap nya tidak besar
- Tidak ada tanda overfitting (yaitu training loss jauh lebih kecil dari validation loss).
- Validation MAE juga masih cukup rendah.

**Kesimpulan:**
- Model LSTM Tidak Overfitting dan Underfitting
- Model baik dan bisa digunakan untuk prediksi

**Perbandingan**
"""

mae_scores = {
    'LSTM Train': train_score_lstm[1],
    'LSTM Val': val_score_lstm[1],
    'LSTM with Tunning Train': train_score_lstm_tunning[1],
    'LSTM with Tunning Val': val_score_lstm_tunning[1]
}

plt.figure(figsize=(12, 5))
plt.bar(mae_scores.keys(), mae_scores.values(), color=['skyblue', 'blue', 'pink', 'red'])
plt.ylabel('MAE')
plt.title('Perbandingan MAE LSTM vs MAE LSTM With Tunning (Train & Validation)')
plt.ylim(0, max(mae_scores.values()) + 0.01)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""Model LSTM with Tunning memiliki MAE yang lebih rendah dari LSTM without tunning baik dari train dan validation. maka LSTM with Tunning model yang paling baik untuk prediksi

# Result

**Melakukan prediksi selama 60 hari kedepan**
"""

# Ambil 60 data terakhir dari data asli (seluruh price yang sudah diskalakan)
last_window = price[-60:]
last_window = np.expand_dims(last_window, axis=(0, 2))

# Lakukan prediksi ke depan 60 hari menggunkan model cnn
future_pred_scaled = best_model.predict(last_window)
future_pred_scaled = future_pred_scaled.flatten()

# Inverse hasil prediksi ke skala asli
future_pred = scaler.inverse_transform(future_pred_scaled.reshape(-1, 1)).flatten()

# Buat range tanggal prediksi 60 hari ke depan
last_date = pd.to_datetime(data_ihsg["Date"].iloc[-1])
future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 61)]

# Simpan hasil prediksi ke DataFrame
future_df = pd.DataFrame({
    "Date": future_dates,
    "Predicted_IHSG": future_pred
})

future_df

"""**Visualisasi Hasil**"""

# Inverse -- cukup 1x run
data_ihsg["Price"] = scaler.inverse_transform(data_ihsg[["Price"]])

# Ambil 60 hari terakhir data historis
historical_60 = data_ihsg.tail(60).copy()

# Gabungkan semua tanggal (historis dan prediksi)
all_dates = pd.concat([historical_60['Date'], future_df['Date']])

plt.figure(figsize=(20, 5))
plt.plot(historical_60['Date'], historical_60['Price'], label='Historical IHSG (60 days)', marker='o')
plt.plot(future_df['Date'], future_df['Predicted_IHSG'], label='Predicted IHSG (60 days)', marker='o')

plt.xlabel('Date')
plt.ylabel('IHSG Price')
plt.title('Historical and Predicted IHSG Prices')
plt.legend()
plt.grid(True)

# Tampilkan semua tanggal pada sumbu X secara jelas
plt.xticks(ticks=all_dates, labels=all_dates.dt.strftime('%Y-%m-%d'), rotation=90)

plt.tight_layout()
plt.show()

"""pada line orange merupakan Hasil dari Prediksi selama 60 hari"""

# Ambil 60 hari terakhir data historis (harga asli)
# historical_60 = data_ihsg.tail(60).copy()

# Data prediksi 60 hari ke depan sudah ada di future_df (kolom "Predicted_IHSG")
# Plot data historis dan prediksi
plt.figure(figsize=(25, 4))
plt.plot(data_ihsg['Date'], data_ihsg['Price'], label='Historical IHSG')
plt.plot(future_df['Date'], future_df['Predicted_IHSG'], label='Predicted IHSG (60 days)')
plt.xlabel('Date')
plt.ylabel('IHSG Price')
plt.title('Historical and Predicted IHSG Prices')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()